# Production RAG Text Knowledge Agent

A **production-grade Retrieval-Augmented Generation (RAG) backend** that ingests documents, performs **hybrid dense + BM25 retrieval**, and serves grounded answers via a **Flask API** with **structured logging** and **offline evaluation**.

This project is designed to demonstrate **real-world GenAI system design**, not a toy demo.

---

## ğŸ”¹ Key Features

* ğŸ“„ **Document Ingestion** (PDF â†’ text â†’ chunks)
* ğŸ” **Hybrid Retrieval**

  * Dense semantic search (ChromaDB + embeddings)
  * Sparse keyword search (BM25)
* ğŸ¤– **Local LLM Inference**

  * Llama 3 via Ollama (no cloud dependency)
* ğŸŒ **Flask API**

  * Stateless `/query` endpoint
  * Request IDs & latency tracking
* ğŸ“Š **Structured JSON Logging**
* ğŸ§ª **Offline Evaluation**

  * Fixed query set
  * Accuracy & latency metrics

---

## ğŸ§  System Architecture

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   PDF Files  â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Ingestion    â”‚
                â”‚ (PDF Loader) â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Chunking     â”‚
                â”‚ (Overlap)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense Retrieval â”‚          â”‚ Sparse Retrieval   â”‚
â”‚ (ChromaDB +     â”‚          â”‚ (BM25)             â”‚
â”‚ Embeddings)     â”‚          â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Hybrid        â”‚
                â”‚ Retriever     â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Prompt +      â”‚
                â”‚ Context       â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Llama 3       â”‚
                â”‚ (Ollama)      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
RAG/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ rag_engine.py          # Core RAG pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ app.py             # Flask API
â”‚   â”‚   â””â”€â”€ logger.py          # Structured JSON logging
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/             # PDF loading & chunking
â”‚   â”œâ”€â”€ indexing/              # ChromaDB + BM25
â”‚   â”œâ”€â”€ retrieval/             # Hybrid retriever
â”‚   â””â”€â”€ generation/            # Prompting + LLM
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Input documents (PDFs)
â”‚   â””â”€â”€ processed/             # Vector store persistence
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval_set.json          # Offline evaluation queries
â”‚   â””â”€â”€ evaluate.py            # Evaluation runner
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-repo-url>
cd RAG
```

---

### 2ï¸âƒ£ Create Virtual Environment (Python 3.11)

```bash
python3.11 -m venv venv
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Install & Start Ollama

Install Ollama (macOS via Homebrew):

```bash
brew install ollama
```

Start the Ollama server:

```bash
ollama serve
```

In a new terminal, pull the model:

```bash
ollama pull llama3
```

---

### 5ï¸âƒ£ Add Documents

Place PDFs inside:

```
data/raw/
```

Example:

```
data/raw/sample.pdf
```

---

## ğŸš€ Running the API

From the project root:

```bash
python -m backend.api.app
```

API runs at:

```
http://127.0.0.1:8000
```

---

### ğŸ”— Query Endpoint

**POST** `/query`

#### Example request:

```bash
curl -X POST http://127.0.0.1:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is this document about?"}'
```

#### Example response:

```json
{
  "request_id": "0935766c-f08b-480e-a756-9c2d8dbf1222",
  "answer": "This document is a resume for an AI Engineer...",
  "latency_seconds": 24.34
}
```

---

## ğŸ“Š Logging & Observability

* Logs are **JSON-formatted**
* Each request includes:

  * Request ID
  * Latency
  * Retrieval metadata
* Logs are emitted to stdout (Docker-friendly)

Example log:

```json
{
  "timestamp": "2025-12-29T08:41:22Z",
  "level": "INFO",
  "message": "request_completed request_id=0935766c latency_seconds=24.34 chunks=5",
  "logger": "rag-api"
}
```

---

## ğŸ§ª Offline Evaluation

Run evaluation as a module:

```bash
python -m evaluation.evaluate
```

### Metrics Produced

* Accuracy (keyword-based grounding)
* Average latency
* Per-query diagnostics

Example output:

```
Accuracy: 1.00
Average Latency: 16.47s
```

This allows **regression testing** and retrieval quality tracking over time.

---

## ğŸ› ï¸ Design Decisions

* **Hybrid retrieval** improves factual grounding vs embeddings alone
* **Strict prompt grounding** prevents hallucinations
* **Local LLM (Ollama)** enables private, offline inference
* **Offline evaluation** avoids subjective LLM-as-judge metrics

---


## ğŸ“„ License

MIT License

---
